{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_full = pd.read_csv(\"../Data/data_train_full_preprocessed.csv\",sep=\";\")\n",
    "data_test_full = pd.read_csv(\"../Data/data_test_full_preprocessed.csv\",sep=\";\")\n",
    "data_val_full = pd.read_csv(\"../Data/data_val_full_preprocessed.csv\",sep=\";\")\n",
    "\n",
    "data_train_nostopword = pd.read_csv(\"../Data/data_train_nostopword_preprocessed.csv\",sep=\";\")\n",
    "data_test_nostopword = pd.read_csv(\"../Data/data_test_nostopword_preprocessed.csv\",sep=\";\")\n",
    "data_val_nostopword = pd.read_csv(\"../Data/data_val_nostopword_preprocessed.csv\",sep=\";\")\n",
    "\n",
    "data_train_nostemstop = pd.read_csv(\"../Data/data_train_nostemstop_preprocessed.csv\",sep=\";\")\n",
    "data_test_nostemstop = pd.read_csv(\"../Data/data_test_nostemstop_preprocessed.csv\",sep=\";\")\n",
    "data_val_nostemstop = pd.read_csv(\"../Data/data_val_nostemstop_preprocessed.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_trans_full = pd.read_csv(\"../Data/data_train_trans_full_preprocessed.csv\",sep=\";\")\n",
    "data_test_trans_full = pd.read_csv(\"../Data/data_test_trans_full_preprocessed.csv\",sep=\";\")\n",
    "data_val_trans_full = pd.read_csv(\"../Data/data_val_trans_full_preprocessed.csv\",sep=\";\")\n",
    "\n",
    "data_train_trans_nostopword = pd.read_csv(\"../Data/data_train_trans_nostopword_preprocessed.csv\",sep=\";\")\n",
    "data_test_trans_nostopword = pd.read_csv(\"../Data/data_test_trans_nostopword_preprocessed.csv\",sep=\";\")\n",
    "data_val_trans_nostopword = pd.read_csv(\"../Data/data_val_trans_nostopword_preprocessed.csv\",sep=\";\")\n",
    "\n",
    "data_train_trans_nostemstop = pd.read_csv(\"../Data/data_train_trans_nostemstop_preprocessed.csv\",sep=\";\")\n",
    "data_test_trans_nostemstop = pd.read_csv(\"../Data/data_test_trans_nostemstop_preprocessed.csv\",sep=\";\")\n",
    "data_val_trans_nostemstop = pd.read_csv(\"../Data/data_val_trans_nostemstop_preprocessed.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Target Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = data_train_full['Tweet_Parsed']\n",
    "X_test_full = data_test_full['Tweet_Parsed']\n",
    "X_val_full = data_val_full['Tweet_Parsed']\n",
    "y_train_full = data_train_full.drop(['Tweet','Tweet_Parsed'],axis=1)\n",
    "y_test_full = data_test_full.drop(['Tweet','Tweet_Parsed'],axis=1)\n",
    "y_val_full = data_val_full.drop(['Tweet','Tweet_Parsed'],axis=1)\n",
    "\n",
    "X_train_nostopword = data_train_nostopword['Tweet_Parsed']\n",
    "X_test_nostopword = data_test_nostopword['Tweet_Parsed']\n",
    "X_val_nostopword = data_val_nostopword['Tweet_Parsed']\n",
    "y_train_nostopword = data_train_nostopword.drop(['Tweet','Tweet_Parsed'],axis=1)\n",
    "y_test_nostopword = data_test_nostopword.drop(['Tweet','Tweet_Parsed'],axis=1)\n",
    "y_val_nostopword = data_val_nostopword.drop(['Tweet','Tweet_Parsed'],axis=1)\n",
    "\n",
    "X_train_nostemstop = data_train_nostemstop['Tweet_Parsed']\n",
    "X_test_nostemstop = data_test_nostemstop['Tweet_Parsed']\n",
    "X_val_nostemstop = data_val_nostemstop['Tweet_Parsed']\n",
    "y_train_nostemstop = data_train_nostemstop.drop(['Tweet','Tweet_Parsed'],axis=1)\n",
    "y_test_nostemstop = data_test_nostemstop.drop(['Tweet','Tweet_Parsed'],axis=1)\n",
    "y_val_nostemstop = data_val_nostemstop.drop(['Tweet','Tweet_Parsed'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trans_full = data_train_trans_full['Tweet_Parsed']\n",
    "X_test_trans_full = data_test_trans_full['Tweet_Parsed']\n",
    "X_val_trans_full = data_val_trans_full['Tweet_Parsed']\n",
    "y_train_trans_full = data_train_trans_full.drop(['Tweet','Tweet_Parsed'],axis=1)\n",
    "y_test_trans_full = data_test_trans_full.drop(['Tweet','Tweet_Parsed'],axis=1)\n",
    "y_val_trans_full = data_val_trans_full.drop(['Tweet','Tweet_Parsed'],axis=1)\n",
    "\n",
    "X_train_trans_nostopword = data_train_trans_nostopword['Tweet_Parsed']\n",
    "X_test_trans_nostopword = data_test_trans_nostopword['Tweet_Parsed']\n",
    "X_val_trans_nostopword = data_val_trans_nostopword['Tweet_Parsed']\n",
    "y_train_trans_nostopword = data_train_trans_nostopword.drop(['Tweet','Tweet_Parsed'],axis=1)\n",
    "y_test_trans_nostopword = data_test_trans_nostopword.drop(['Tweet','Tweet_Parsed'],axis=1)\n",
    "y_val_trans_nostopword = data_val_trans_nostopword.drop(['Tweet','Tweet_Parsed'],axis=1)\n",
    "\n",
    "X_train_trans_nostemstop = data_train_trans_nostemstop['Tweet_Parsed']\n",
    "X_test_trans_nostemstop = data_test_trans_nostemstop['Tweet_Parsed']\n",
    "X_val_trans_nostemstop = data_val_trans_nostemstop['Tweet_Parsed']\n",
    "y_train_trans_nostemstop = data_train_trans_nostemstop.drop(['Tweet','Tweet_Parsed'],axis=1)\n",
    "y_test_trans_nostemstop = data_test_trans_nostemstop.drop(['Tweet','Tweet_Parsed'],axis=1)\n",
    "y_val_trans_nostemstop = data_val_trans_nostemstop.drop(['Tweet','Tweet_Parsed'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(X_train_,X_test_,y_train_,y_test_,X_val_,y_val_,target,max_features = 5000):\n",
    "    ngram_range = (1,1)\n",
    "    min_df = 1\n",
    "    max_df = 1.\n",
    "    tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2')\n",
    "    features_train = tfidf.fit_transform(X_train_).toarray()\n",
    "    labels_train = y_train_.values\n",
    "    features_test = tfidf.transform(X_test_).toarray()\n",
    "    labels_test = y_test_.values\n",
    "    features_val = tfidf.transform(X_val_).toarray()\n",
    "    labels_val = y_val_.values\n",
    "    features_all = tfidf.get_feature_names()\n",
    "    data_train_tfidf = pd.DataFrame(data=features_train,columns=features_all)\n",
    "    data_train_tfidf = data_train_tfidf.join(pd.DataFrame(data=labels_train,columns=target))\n",
    "    data_test_tfidf = pd.DataFrame(data=features_test,columns=features_all)\n",
    "    data_test_tfidf = data_test_tfidf.join(pd.DataFrame(data=labels_test,columns=target))\n",
    "    data_val_tfidf = pd.DataFrame(data=features_val,columns=features_all)\n",
    "    data_val_tfidf = data_val_tfidf.join(pd.DataFrame(data=labels_val,columns=target))\n",
    "    return data_train_tfidf,data_test_tfidf,data_val_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data_train_full.drop(['Tweet','Tweet_Parsed'],axis = 1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Without Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_full_tfidf,data_test_full_tfidf,data_val_full_tfidf = tfidf(X_train_full,X_test_full,y_train_full,y_test_full,X_val_full,y_val_full,target,5000)\n",
    "data_train_nostopword_tfidf,data_test_nostopword_tfidf,data_val_nostopword_tfidf = tfidf(X_train_nostopword,X_test_nostopword,y_train_nostopword,y_test_nostopword,X_val_nostopword,y_val_nostopword,target,5000)\n",
    "data_train_nostemstop_tfidf,data_test_nostemstop_tfidf,data_val_nostemstop_tfidf = tfidf(X_train_nostemstop,X_test_nostemstop,y_train_nostemstop,y_test_nostemstop,X_val_nostemstop,y_val_nostemstop,target,5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data With Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_trans_full_tfidf,data_test_trans_full_tfidf,data_val_trans_full_tfidf = tfidf(X_train_trans_full,X_test_trans_full,y_train_trans_full,y_test_trans_full,X_val_trans_full,y_val_trans_full,target,5000)\n",
    "data_train_trans_nostopword_tfidf,data_test_trans_nostopword_tfidf,data_val_trans_nostopword_tfidf = tfidf(X_train_trans_nostopword,X_test_trans_nostopword,y_train_trans_nostopword,y_test_trans_nostopword,X_val_trans_nostopword,y_val_trans_nostopword,target,5000)\n",
    "data_train_trans_nostemstop_tfidf,data_test_trans_nostemstop_tfidf,data_val_trans_nostemstop_tfidf = tfidf(X_train_trans_nostemstop,X_test_trans_nostemstop,y_train_trans_nostemstop,y_test_trans_nostemstop,X_val_trans_nostemstop,y_val_trans_nostemstop,target,5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(s):\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.str.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = np.concatenate(tokens)\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data With Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokensTest = generate_ngrams(data_test['Tweet_Parsed']).tolist()\n",
    "# tokensTrain = np.unique(tokensTrain).tolist()\n",
    "tokensTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outOfVocab = []\n",
    "for i in tokensTest:\n",
    "    if i not in fitur:\n",
    "        outOfVocab.append(i)\n",
    "outOfVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# out of vocabulary : \", len(outOfVocab))\n",
    "print(\"# out of vocabulary (unique words) : \", len(np.unique(outOfVocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data With Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokensTestTrans = generate_ngrams(data_test_trans['Tweet_Parsed']).tolist()\n",
    "# tokensTrain = np.unique(tokensTrain).tolist()\n",
    "tokensTestTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outOfVocabTrans = []\n",
    "for i in tokensTestTrans:\n",
    "    if i not in fitur_trans:\n",
    "        outOfVocabTrans.append(i)\n",
    "outOfVocabTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# out of vocabulary : \", len(outOfVocabTrans))\n",
    "print(\"# out of vocabulary (unique words) : \", len(np.unique(outOfVocabTrans)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_full_tfidf.to_csv('../Data/data_train_full_tfidf.csv', index = False)\n",
    "data_test_full_tfidf.to_csv('../Data/data_test_full_tfidf.csv', index = False)\n",
    "data_val_full_tfidf.to_csv('../Data/data_val_full_tfidf.csv', index = False)\n",
    "\n",
    "data_train_nostopword_tfidf.to_csv('../Data/data_train_nostopword_tfidf.csv', index = False)\n",
    "data_test_nostopword_tfidf.to_csv('../Data/data_test_nostopword_tfidf.csv', index = False)\n",
    "data_val_nostopword_tfidf.to_csv('../Data/data_val_nostopword_tfidf.csv', index = False)\n",
    "\n",
    "data_train_nostemstop_tfidf.to_csv('../Data/data_train_nostemstop_tfidf.csv', index = False)\n",
    "data_test_nostemstop_tfidf.to_csv('../Data/data_test_nostemstop_tfidf.csv', index = False)\n",
    "data_val_nostemstop_tfidf.to_csv('../Data/data_val_nostemstop_tfidf.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_trans_full_tfidf.to_csv('../Data/data_train_trans_full_tfidf.csv', index = False)\n",
    "data_test_trans_full_tfidf.to_csv('../Data/data_test_trans_full_tfidf.csv', index = False)\n",
    "data_val_trans_full_tfidf.to_csv('../Data/data_val_trans_full_tfidf.csv', index = False)\n",
    "\n",
    "data_train_trans_nostopword_tfidf.to_csv('../Data/data_train_trans_nostopword_tfidf.csv', index = False)\n",
    "data_test_trans_nostopword_tfidf.to_csv('../Data/data_test_trans_nostopword_tfidf.csv', index = False)\n",
    "data_val_trans_nostopword_tfidf.to_csv('../Data/data_val_trans_nostopword_tfidf.csv', index = False)\n",
    "\n",
    "data_train_trans_nostemstop_tfidf.to_csv('../Data/data_train_trans_nostemstop_tfidf.csv', index = False)\n",
    "data_test_trans_nostemstop_tfidf.to_csv('../Data/data_test_trans_nostemstop_tfidf.csv', index = False)\n",
    "data_val_trans_nostemstop_tfidf.to_csv('../Data/data_val_trans_nostemstop_tfidf.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
